activate venv

go to spider folder

scrapy genspider bookspider books.toscrape.com ## name is bookspider and url is books.toscrape.com

for css selector use scrapy shell and do pip install ipython it is a shell thats easier to read

go to scrapy.cfg and add shell: ipython

run scrapy shell in terminal

fetch('url')

response.css('article.product_pod' ) ## tags 
response.css('article.product_pod').get() ## gets html of first tag

books = response.css('article.product_pod' )

book = books[0]

In [8]: book.css('h3 a::text').get()
Out[8]: 'A Light in the ...'

In [9]: book.css('.product_price .price_color::text').get()
Out[9]: 'Â£51.77'

In [15]: book.css('h3 a').attrib['href']
Out[15]: 'catalogue/a-light-in-the-attic_1000/index.html'

#scrapy shell is used to identify th css selectors then exit it by using exit command

go one folder up from spiders parent folder and run "scrapy crawl spider_name"

now to scrap next pages of the same website open "scrapy shell"
see where the next page name is stored in inspect window
In [4]: response.css('li.next a::attr(href)' ).get()
Out[4]: 'catalogue/page-2.html'

response.css('article.product_page')

response.css('.product_main h1::text').get()

In [12]: response.xpath("//ul[@class='breadcrumb']/li[@class='active']/preceding-sibling::li[1]/a/text()").get()   ##ul->class = breadcrumb->li active-> go to preceding sibling li[1] ->text  
Out[12]: 'Religion'

In [14]: response.xpath("//div[@id='product_description']/following-sibling::p/text()").get()
gets product description



In [15]: table_rows = response.css("table tr")

In [16]: len(table_rows)
Out[16]: 7

In [17]: table_rows[1].css('td::text').get()
Out[17]: 'Books'

In [18]: table_rows[1].css('th::text').get()
Out[18]: 'Product Type'


In [19]: response.css('p.star-rating').attrib['class']
Out[19]: 'star-rating Four'


for cleaning data use pipelines and enable them in setting.py

scrapy crawl -O data.csv -> adds all the data again if file exists
scrapy crawl -o data.csv -> add data after the data that is stored in file

Add feed to setting.py to specify the format of data to be stored in
after adding FEED to setting.py you can just run srcaper using 'scrapy crawl bookspider' 


PART 8
Fake User Agents & Browser Headers
1) Why are we getting blocked
2) Explaining and using user agents to byPass getting blocked
3) explaining and using request headers to byPass getting blocked

setup user agent in settings.py
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'
but this is not sufficient as you are same user making request again and again.

use middleware to rotate through all the fake user agents 
use scrapops.io to get API key for fake user agents

after writing user agent class in middle ware enable it in settings.py

for complex scraping set robots_obey to false 

we can either replace just the user agent in header or we can replace the whole header 
cerating new middleware for it